{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n",
      "0 0.6900227665901184\n",
      "1 0.6853911876678467\n",
      "2 0.6812512278556824\n",
      "3 0.6775501370429993\n",
      "4 0.6742392182350159\n",
      "5 0.6712766885757446\n",
      "6 0.668624222278595\n",
      "7 0.6662486791610718\n",
      "8 0.6641209125518799\n",
      "9 0.6622132062911987\n",
      "10 0.6605024337768555\n",
      "11 0.6589677929878235\n",
      "12 0.6575902700424194\n",
      "13 0.6563538312911987\n",
      "14 0.6552429795265198\n",
      "15 0.6542451977729797\n",
      "16 0.6533478498458862\n",
      "17 0.6525409817695618\n",
      "18 0.6518154740333557\n",
      "19 0.6511630415916443\n",
      "20 0.6505750417709351\n",
      "21 0.6500465869903564\n",
      "22 0.6495700478553772\n",
      "23 0.6491405367851257\n",
      "24 0.6487540602684021\n",
      "25 0.6484049558639526\n",
      "26 0.6480908393859863\n",
      "27 0.6478073000907898\n",
      "28 0.6475521326065063\n",
      "29 0.6473208665847778\n",
      "30 0.6471126079559326\n",
      "31 0.6469246745109558\n",
      "32 0.6467548608779907\n",
      "33 0.6466018557548523\n",
      "34 0.64646315574646\n",
      "35 0.6463384628295898\n",
      "36 0.6462252140045166\n",
      "37 0.6461225748062134\n",
      "38 0.6460304856300354\n",
      "39 0.6459470987319946\n",
      "40 0.6458712220191956\n",
      "41 0.6458028554916382\n",
      "42 0.6457411050796509\n",
      "43 0.6456849575042725\n",
      "44 0.6456343531608582\n",
      "45 0.6455885767936707\n",
      "46 0.6455470323562622\n",
      "47 0.6455091238021851\n",
      "48 0.6454747319221497\n",
      "49 0.645443856716156\n",
      "50 0.6454157829284668\n",
      "51 0.6453900337219238\n",
      "52 0.6453671455383301\n",
      "53 0.6453458666801453\n",
      "54 0.6453267335891724\n",
      "55 0.6453096270561218\n",
      "56 0.6452942490577698\n",
      "57 0.645279586315155\n",
      "58 0.645266592502594\n",
      "59 0.6452542543411255\n",
      "60 0.6452438235282898\n",
      "61 0.6452333331108093\n",
      "62 0.6452245712280273\n",
      "63 0.6452159285545349\n",
      "64 0.6452087759971619\n",
      "65 0.6452021598815918\n",
      "66 0.6451955437660217\n",
      "67 0.645189642906189\n",
      "68 0.6451842784881592\n",
      "69 0.6451794505119324\n",
      "70 0.6451746821403503\n",
      "71 0.6451705098152161\n",
      "72 0.6451669931411743\n",
      "73 0.6451634764671326\n",
      "74 0.645159900188446\n",
      "75 0.645156979560852\n",
      "76 0.6451539993286133\n",
      "77 0.6451515555381775\n",
      "78 0.6451489925384521\n",
      "79 0.6451466679573059\n",
      "80 0.6451441645622253\n",
      "81 0.6451423168182373\n",
      "82 0.6451402306556702\n",
      "83 0.6451388597488403\n",
      "84 0.6451367139816284\n",
      "85 0.6451350450515747\n",
      "86 0.6451337337493896\n",
      "87 0.6451318860054016\n",
      "88 0.6451307535171509\n",
      "89 0.6451291441917419\n",
      "90 0.6451278328895569\n",
      "91 0.6451261639595032\n",
      "92 0.6451255679130554\n",
      "93 0.6451243162155151\n",
      "94 0.645122766494751\n",
      "95 0.6451218724250793\n",
      "96 0.6451210379600525\n",
      "97 0.6451200246810913\n",
      "98 0.6451184749603271\n",
      "99 0.6451177597045898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape)\n",
    "print(y_data.data.shape)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6)\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "\n",
    "\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "  \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6526],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6521],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6528],\n",
      "        [ 0.6529],\n",
      "        [ 0.6523],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6522],\n",
      "        [ 0.6528],\n",
      "        [ 0.6526],\n",
      "        [ 0.6522],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6521],\n",
      "        [ 0.6527],\n",
      "        [ 0.6532],\n",
      "        [ 0.6534],\n",
      "        [ 0.6525],\n",
      "        [ 0.6536],\n",
      "        [ 0.6537],\n",
      "        [ 0.6524],\n",
      "        [ 0.6522],\n",
      "        [ 0.6531],\n",
      "        [ 0.6538],\n",
      "        [ 0.6530],\n",
      "        [ 0.6530],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6524],\n",
      "        [ 0.6523],\n",
      "        [ 0.6516],\n",
      "        [ 0.6538],\n",
      "        [ 0.6522],\n",
      "        [ 0.6532],\n",
      "        [ 0.6525],\n",
      "        [ 0.6531],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6528],\n",
      "        [ 0.6526],\n",
      "        [ 0.6527],\n",
      "        [ 0.6539],\n",
      "        [ 0.6537],\n",
      "        [ 0.6535],\n",
      "        [ 0.6529],\n",
      "        [ 0.6527],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6517],\n",
      "        [ 0.6517],\n",
      "        [ 0.6529],\n",
      "        [ 0.6520],\n",
      "        [ 0.6527],\n",
      "        [ 0.6522],\n",
      "        [ 0.6535],\n",
      "        [ 0.6525],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6514],\n",
      "        [ 0.6539],\n",
      "        [ 0.6536],\n",
      "        [ 0.6530],\n",
      "        [ 0.6535],\n",
      "        [ 0.6521],\n",
      "        [ 0.6533],\n",
      "        [ 0.6527],\n",
      "        [ 0.6535],\n",
      "        [ 0.6521],\n",
      "        [ 0.6526],\n",
      "        [ 0.6526],\n",
      "        [ 0.6534],\n",
      "        [ 0.6539],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6535],\n",
      "        [ 0.6519],\n",
      "        [ 0.6529],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6532],\n",
      "        [ 0.6534],\n",
      "        [ 0.6525],\n",
      "        [ 0.6533],\n",
      "        [ 0.6520],\n",
      "        [ 0.6526],\n",
      "        [ 0.6532],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6537],\n",
      "        [ 0.6534],\n",
      "        [ 0.6519],\n",
      "        [ 0.6522],\n",
      "        [ 0.6533],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6519],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6534],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6536],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6524],\n",
      "        [ 0.6534],\n",
      "        [ 0.6522],\n",
      "        [ 0.6526],\n",
      "        [ 0.6524],\n",
      "        [ 0.6532],\n",
      "        [ 0.6533],\n",
      "        [ 0.6522],\n",
      "        [ 0.6529],\n",
      "        [ 0.6531],\n",
      "        [ 0.6523],\n",
      "        [ 0.6528],\n",
      "        [ 0.6519],\n",
      "        [ 0.6527],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6521],\n",
      "        [ 0.6538],\n",
      "        [ 0.6524],\n",
      "        [ 0.6533],\n",
      "        [ 0.6524],\n",
      "        [ 0.6537],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6527],\n",
      "        [ 0.6525],\n",
      "        [ 0.6533],\n",
      "        [ 0.6526],\n",
      "        [ 0.6535],\n",
      "        [ 0.6526],\n",
      "        [ 0.6524],\n",
      "        [ 0.6525],\n",
      "        [ 0.6522],\n",
      "        [ 0.6534],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6525],\n",
      "        [ 0.6522],\n",
      "        [ 0.6531],\n",
      "        [ 0.6534],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6530],\n",
      "        [ 0.6525],\n",
      "        [ 0.6527],\n",
      "        [ 0.6527],\n",
      "        [ 0.6526],\n",
      "        [ 0.6530],\n",
      "        [ 0.6534],\n",
      "        [ 0.6525],\n",
      "        [ 0.6526],\n",
      "        [ 0.6535],\n",
      "        [ 0.6526],\n",
      "        [ 0.6533],\n",
      "        [ 0.6529],\n",
      "        [ 0.6522],\n",
      "        [ 0.6530],\n",
      "        [ 0.6532],\n",
      "        [ 0.6522],\n",
      "        [ 0.6516],\n",
      "        [ 0.6521],\n",
      "        [ 0.6521],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6530],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6524],\n",
      "        [ 0.6531],\n",
      "        [ 0.6533],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6530],\n",
      "        [ 0.6520],\n",
      "        [ 0.6533],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6538],\n",
      "        [ 0.6526],\n",
      "        [ 0.6535],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6532],\n",
      "        [ 0.6538],\n",
      "        [ 0.6524],\n",
      "        [ 0.6533],\n",
      "        [ 0.6530],\n",
      "        [ 0.6524],\n",
      "        [ 0.6526],\n",
      "        [ 0.6531],\n",
      "        [ 0.6531],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6527],\n",
      "        [ 0.6528],\n",
      "        [ 0.6533],\n",
      "        [ 0.6526],\n",
      "        [ 0.6523],\n",
      "        [ 0.6528],\n",
      "        [ 0.6520],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6535],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6530],\n",
      "        [ 0.6521],\n",
      "        [ 0.6530],\n",
      "        [ 0.6523],\n",
      "        [ 0.6523],\n",
      "        [ 0.6532],\n",
      "        [ 0.6526],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6530],\n",
      "        [ 0.6533],\n",
      "        [ 0.6531],\n",
      "        [ 0.6527],\n",
      "        [ 0.6530],\n",
      "        [ 0.6532],\n",
      "        [ 0.6531],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6530],\n",
      "        [ 0.6532],\n",
      "        [ 0.6526],\n",
      "        [ 0.6529],\n",
      "        [ 0.6536],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6523],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6539],\n",
      "        [ 0.6539],\n",
      "        [ 0.6528],\n",
      "        [ 0.6528],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6527],\n",
      "        [ 0.6526],\n",
      "        [ 0.6523],\n",
      "        [ 0.6522],\n",
      "        [ 0.6531],\n",
      "        [ 0.6518],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6519],\n",
      "        [ 0.6524],\n",
      "        [ 0.6522],\n",
      "        [ 0.6534],\n",
      "        [ 0.6536],\n",
      "        [ 0.6523],\n",
      "        [ 0.6535],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6523],\n",
      "        [ 0.6524],\n",
      "        [ 0.6521],\n",
      "        [ 0.6538],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6530],\n",
      "        [ 0.6522],\n",
      "        [ 0.6526],\n",
      "        [ 0.6530],\n",
      "        [ 0.6532],\n",
      "        [ 0.6532],\n",
      "        [ 0.6533],\n",
      "        [ 0.6529],\n",
      "        [ 0.6523],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6525],\n",
      "        [ 0.6518],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6537],\n",
      "        [ 0.6537],\n",
      "        [ 0.6532],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6528],\n",
      "        [ 0.6538],\n",
      "        [ 0.6536],\n",
      "        [ 0.6522],\n",
      "        [ 0.6532],\n",
      "        [ 0.6539],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6532],\n",
      "        [ 0.6536],\n",
      "        [ 0.6531],\n",
      "        [ 0.6534],\n",
      "        [ 0.6534],\n",
      "        [ 0.6524],\n",
      "        [ 0.6539],\n",
      "        [ 0.6528],\n",
      "        [ 0.6531],\n",
      "        [ 0.6525],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6533],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6537],\n",
      "        [ 0.6528],\n",
      "        [ 0.6522],\n",
      "        [ 0.6524],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6538],\n",
      "        [ 0.6531],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6519],\n",
      "        [ 0.6526],\n",
      "        [ 0.6537],\n",
      "        [ 0.6532],\n",
      "        [ 0.6538],\n",
      "        [ 0.6523],\n",
      "        [ 0.6521],\n",
      "        [ 0.6528],\n",
      "        [ 0.6522],\n",
      "        [ 0.6535],\n",
      "        [ 0.6515],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6527],\n",
      "        [ 0.6526],\n",
      "        [ 0.6531],\n",
      "        [ 0.6535],\n",
      "        [ 0.6526],\n",
      "        [ 0.6518],\n",
      "        [ 0.6518],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6529],\n",
      "        [ 0.6533],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6520],\n",
      "        [ 0.6524],\n",
      "        [ 0.6526],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6523],\n",
      "        [ 0.6538],\n",
      "        [ 0.6527],\n",
      "        [ 0.6523],\n",
      "        [ 0.6520],\n",
      "        [ 0.6530],\n",
      "        [ 0.6538],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6537],\n",
      "        [ 0.6540],\n",
      "        [ 0.6528],\n",
      "        [ 0.6521],\n",
      "        [ 0.6528],\n",
      "        [ 0.6530],\n",
      "        [ 0.6527],\n",
      "        [ 0.6525],\n",
      "        [ 0.6532],\n",
      "        [ 0.6537],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6529],\n",
      "        [ 0.6535],\n",
      "        [ 0.6526],\n",
      "        [ 0.6529],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6523],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6533],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6528],\n",
      "        [ 0.6536],\n",
      "        [ 0.6523],\n",
      "        [ 0.6530],\n",
      "        [ 0.6522],\n",
      "        [ 0.6527],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6527],\n",
      "        [ 0.6533],\n",
      "        [ 0.6528],\n",
      "        [ 0.6526],\n",
      "        [ 0.6531],\n",
      "        [ 0.6530],\n",
      "        [ 0.6533],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6529],\n",
      "        [ 0.6533],\n",
      "        [ 0.6522],\n",
      "        [ 0.6532],\n",
      "        [ 0.6531],\n",
      "        [ 0.6534],\n",
      "        [ 0.6524],\n",
      "        [ 0.6534],\n",
      "        [ 0.6513],\n",
      "        [ 0.6535],\n",
      "        [ 0.6527],\n",
      "        [ 0.6531],\n",
      "        [ 0.6537],\n",
      "        [ 0.6537],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6522],\n",
      "        [ 0.6529],\n",
      "        [ 0.6535],\n",
      "        [ 0.6523],\n",
      "        [ 0.6532],\n",
      "        [ 0.6527],\n",
      "        [ 0.6535],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6537],\n",
      "        [ 0.6538],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6533],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6530],\n",
      "        [ 0.6530],\n",
      "        [ 0.6525],\n",
      "        [ 0.6537],\n",
      "        [ 0.6534],\n",
      "        [ 0.6526],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6521],\n",
      "        [ 0.6521],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6527],\n",
      "        [ 0.6529],\n",
      "        [ 0.6532],\n",
      "        [ 0.6537],\n",
      "        [ 0.6531],\n",
      "        [ 0.6532],\n",
      "        [ 0.6525],\n",
      "        [ 0.6521],\n",
      "        [ 0.6528],\n",
      "        [ 0.6523],\n",
      "        [ 0.6523],\n",
      "        [ 0.6535],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6537],\n",
      "        [ 0.6525],\n",
      "        [ 0.6527],\n",
      "        [ 0.6537],\n",
      "        [ 0.6538],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6534],\n",
      "        [ 0.6532],\n",
      "        [ 0.6531],\n",
      "        [ 0.6528],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6530],\n",
      "        [ 0.6534],\n",
      "        [ 0.6538],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6535],\n",
      "        [ 0.6521],\n",
      "        [ 0.6519],\n",
      "        [ 0.6526],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6519],\n",
      "        [ 0.6529],\n",
      "        [ 0.6523],\n",
      "        [ 0.6522],\n",
      "        [ 0.6532],\n",
      "        [ 0.6521],\n",
      "        [ 0.6529],\n",
      "        [ 0.6529],\n",
      "        [ 0.6533],\n",
      "        [ 0.6525],\n",
      "        [ 0.6535],\n",
      "        [ 0.6528],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6529],\n",
      "        [ 0.6522],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6524],\n",
      "        [ 0.6524],\n",
      "        [ 0.6523],\n",
      "        [ 0.6526],\n",
      "        [ 0.6524],\n",
      "        [ 0.6534],\n",
      "        [ 0.6527],\n",
      "        [ 0.6535],\n",
      "        [ 0.6525],\n",
      "        [ 0.6535],\n",
      "        [ 0.6523],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6532],\n",
      "        [ 0.6522],\n",
      "        [ 0.6526],\n",
      "        [ 0.6534],\n",
      "        [ 0.6533],\n",
      "        [ 0.6530],\n",
      "        [ 0.6529],\n",
      "        [ 0.6534],\n",
      "        [ 0.6516],\n",
      "        [ 0.6531],\n",
      "        [ 0.6516],\n",
      "        [ 0.6522],\n",
      "        [ 0.6534],\n",
      "        [ 0.6531],\n",
      "        [ 0.6522],\n",
      "        [ 0.6527],\n",
      "        [ 0.6536],\n",
      "        [ 0.6528],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6530],\n",
      "        [ 0.6518],\n",
      "        [ 0.6526],\n",
      "        [ 0.6523],\n",
      "        [ 0.6527],\n",
      "        [ 0.6526],\n",
      "        [ 0.6539],\n",
      "        [ 0.6516],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6537],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6531],\n",
      "        [ 0.6528],\n",
      "        [ 0.6533],\n",
      "        [ 0.6524],\n",
      "        [ 0.6526],\n",
      "        [ 0.6536],\n",
      "        [ 0.6530],\n",
      "        [ 0.6539],\n",
      "        [ 0.6534],\n",
      "        [ 0.6534],\n",
      "        [ 0.6526],\n",
      "        [ 0.6527],\n",
      "        [ 0.6530],\n",
      "        [ 0.6529],\n",
      "        [ 0.6528],\n",
      "        [ 0.6538],\n",
      "        [ 0.6525],\n",
      "        [ 0.6530],\n",
      "        [ 0.6526],\n",
      "        [ 0.6526],\n",
      "        [ 0.6520],\n",
      "        [ 0.6523],\n",
      "        [ 0.6523],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6524],\n",
      "        [ 0.6530],\n",
      "        [ 0.6527],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6537],\n",
      "        [ 0.6530],\n",
      "        [ 0.6526],\n",
      "        [ 0.6525],\n",
      "        [ 0.6532],\n",
      "        [ 0.6524],\n",
      "        [ 0.6539],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6525],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6539],\n",
      "        [ 0.6536],\n",
      "        [ 0.6533],\n",
      "        [ 0.6536],\n",
      "        [ 0.6535],\n",
      "        [ 0.6532],\n",
      "        [ 0.6531],\n",
      "        [ 0.6529],\n",
      "        [ 0.6532],\n",
      "        [ 0.6531],\n",
      "        [ 0.6536],\n",
      "        [ 0.6519],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6525],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6527],\n",
      "        [ 0.6528],\n",
      "        [ 0.6527],\n",
      "        [ 0.6532],\n",
      "        [ 0.6527],\n",
      "        [ 0.6534],\n",
      "        [ 0.6531],\n",
      "        [ 0.6530],\n",
      "        [ 0.6526],\n",
      "        [ 0.6518],\n",
      "        [ 0.6516],\n",
      "        [ 0.6533],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6528],\n",
      "        [ 0.6535],\n",
      "        [ 0.6532],\n",
      "        [ 0.6527],\n",
      "        [ 0.6526],\n",
      "        [ 0.6526],\n",
      "        [ 0.6519],\n",
      "        [ 0.6531],\n",
      "        [ 0.6532],\n",
      "        [ 0.6533],\n",
      "        [ 0.6535],\n",
      "        [ 0.6521],\n",
      "        [ 0.6526],\n",
      "        [ 0.6524],\n",
      "        [ 0.6527],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529],\n",
      "        [ 0.6531],\n",
      "        [ 0.6538],\n",
      "        [ 0.6531],\n",
      "        [ 0.6533],\n",
      "        [ 0.6520],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6525],\n",
      "        [ 0.6523],\n",
      "        [ 0.6536],\n",
      "        [ 0.6524],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6526],\n",
      "        [ 0.6537],\n",
      "        [ 0.6532],\n",
      "        [ 0.6526],\n",
      "        [ 0.6534],\n",
      "        [ 0.6527],\n",
      "        [ 0.6532],\n",
      "        [ 0.6530],\n",
      "        [ 0.6530],\n",
      "        [ 0.6527],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6523],\n",
      "        [ 0.6532],\n",
      "        [ 0.6527],\n",
      "        [ 0.6522],\n",
      "        [ 0.6524],\n",
      "        [ 0.6529],\n",
      "        [ 0.6533],\n",
      "        [ 0.6534],\n",
      "        [ 0.6528],\n",
      "        [ 0.6534],\n",
      "        [ 0.6530],\n",
      "        [ 0.6528],\n",
      "        [ 0.6533],\n",
      "        [ 0.6523],\n",
      "        [ 0.6528],\n",
      "        [ 0.6531],\n",
      "        [ 0.6525],\n",
      "        [ 0.6526],\n",
      "        [ 0.6520],\n",
      "        [ 0.6523],\n",
      "        [ 0.6534],\n",
      "        [ 0.6536],\n",
      "        [ 0.6524],\n",
      "        [ 0.6526],\n",
      "        [ 0.6529],\n",
      "        [ 0.6522],\n",
      "        [ 0.6517],\n",
      "        [ 0.6534],\n",
      "        [ 0.6531],\n",
      "        [ 0.6526],\n",
      "        [ 0.6527],\n",
      "        [ 0.6534],\n",
      "        [ 0.6529],\n",
      "        [ 0.6530],\n",
      "        [ 0.6523],\n",
      "        [ 0.6528],\n",
      "        [ 0.6523],\n",
      "        [ 0.6524],\n",
      "        [ 0.6524],\n",
      "        [ 0.6532],\n",
      "        [ 0.6528],\n",
      "        [ 0.6530],\n",
      "        [ 0.6523],\n",
      "        [ 0.6529],\n",
      "        [ 0.6536],\n",
      "        [ 0.6525],\n",
      "        [ 0.6529]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(x_data) \n",
    "print(outputs.data)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0645,  0.5259,  0.3984],\n",
      "         [-0.5723, -0.1520,  0.3550]],\n",
      "\n",
      "        [[-0.8736,  0.7260,  0.8774],\n",
      "         [-0.5424,  0.7017,  0.8730]]])\n",
      "tensor([[[-0.5723, -0.1520,  0.3550],\n",
      "         [-0.5424,  0.7017,  0.8730]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "seq_length = 2\n",
    "input_dim = 5\n",
    "hidden_dim = 3\n",
    "batch_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "cell = nn.RNN(input_size = input_dim , hidden_size = hidden_dim , batch_first = True)\n",
    "init_hidden = Variable(torch.randn(num_layers,batch_size,hidden_dim))\n",
    "inputs = [[[1,0,0,0,0],[0,0,1,0,0]] , [ [0,1,1,0,0],[1,1,0,0,0] ] ]\n",
    "inputs = Variable(torch.Tensor(inputs))\n",
    "\n",
    "outputs,hidden = cell(inputs,init_hidden)\n",
    "\n",
    "print(outputs)\n",
    "print(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-45 *\n",
      "       [ 1.4013])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-379f831c19d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm2d\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mym2d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[1;32m--> 759\u001b[1;33m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \"\"\"\n\u001b[1;32m-> 1442\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 944\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Net (nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,seq_length,num_layers,batch_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.RNN(input_size = input_dim , hidden_size = hidden_dim , batch_first = True)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        #x = x.view(batch_size,seq_length,input_dim)\n",
    "        outputs,hidden = self.rnn(x,hidden)\n",
    "        #outputs = outputs.view(-1,self.hidden_dim)\n",
    "        #fin_out = []\n",
    "        #for m2d in outputs:\n",
    "        #    tlis= []\n",
    "        #    for lis in m2d:\n",
    "        #        for ele in lis:\n",
    "        #            if (ele == 1):\n",
    "        #                tlis.append(lis.index(ele))\n",
    "        #    fin_out.append(tlis)\n",
    "                    \n",
    "            \n",
    "        return hidden,outputs\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.randn(self.num_layers,self.batch_size,self.hidden_dim))\n",
    "\n",
    "net = Net(input_dim = 5,hidden_dim =5,seq_length =6,num_layers= 1,batch_size = 1)    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.1)\n",
    "\n",
    "x_data = [[[1,0,0,0,0],[0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0]]]\n",
    "y_data = [[ [0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0],[0,0,0,0,1] ]]\n",
    "\n",
    "x_data = Variable(torch.Tensor(x_data))\n",
    "y_data = Variable(torch.LongTensor(y_data))\n",
    "hidden = net.init_hidden()\n",
    "for epoch in range(100):\n",
    "    hidden,output = net(x_data,hidden)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for m2d,ym2d in zip(output,y_data):\n",
    "        for out,label in zip(m2d,ym2d):\n",
    "            print(torch.Tensor(torch.nonzero(label.data).squeeze().item()))\n",
    "            loss += criterion(out,torch.Tensor(torch.nonzero(label.data).squeeze().item()))\n",
    "            \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"epoch :%d , loss : %.3f\" % (epoch+1 ,loss.item() ))\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  0,  0,  0])\n",
      "tensor([ 1,  0,  0,  0,  0])\n",
      "tensor([ 0,  0,  1,  0,  0])\n",
      "tensor([ 0,  0,  0,  1,  0])\n",
      "tensor([ 0,  0,  0,  1,  0])\n",
      "tensor([ 0,  0,  0,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "x_data = [[[1,0,0,0,0],[0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0]]]\n",
    "y_data = [[ [0,1,0,0,0],[1,0,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,1,0],[0,0,0,0,1] ]]\n",
    "\n",
    "x_data = Variable(torch.Tensor(x_data))\n",
    "y_data = Variable(torch.LongTensor(y_data))\n",
    "for m2d in y_data:\n",
    "    for lis in m2d:\n",
    "        print(lis.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 20])\n",
      "torch.Size([1, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "rnn = nn.GRU(10,20,1,batch_first = True)\n",
    "inp = torch.randn(3,5,10)\n",
    "ho = torch.randn(1,3,20)\n",
    "out,hn = rnn(inp,ho)\n",
    "print(out.size())\n",
    "ln = nn.Linear(20,10)\n",
    "print(ln(hn).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "rn = nn.RNN(10,20,batch_first =True)\n",
    "inp = torch.randn(3,5,10)\n",
    "ho = torch.randn(1,3,20)\n",
    "out,hn = rn(inp,ho)\n",
    "print(out.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
